{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7a9fb2",
   "metadata": {},
   "source": [
    "## Preparing text to use with tensorflow models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eac6d1",
   "metadata": {},
   "source": [
    "### Import classes you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725540fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 22:37:22.998845: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-10 22:37:23.689809: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-10 22:37:23.691062: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-10 22:37:34.906983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcf11b",
   "metadata": {},
   "source": [
    "### Write some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7bbf3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My favourite food is ice cream', 'do you like ice cream too?', 'My dog likes ice cream too!', 'your favourite flavour of ice cream is chocolate', \"chocolate isn't good for dogs\", 'your dog your cat, and your parrot prefer brocolli']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"My favourite food is ice cream\",\n",
    "    \"do you like ice cream too?\",\n",
    "    \"My dog likes ice cream too!\",\n",
    "    \"your favourite flavour of ice cream is chocolate\",\n",
    "    \"chocolate isn't good for dogs\",\n",
    "    \"your dog your cat, and your parrot prefer brocolli\"\n",
    "]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44be0f",
   "metadata": {},
   "source": [
    "### Create a tokenizer and define an out of vocabulary token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be2449d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.text.Tokenizer at 0x7f6f6e77e110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adecf980",
   "metadata": {},
   "source": [
    "### Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8d2b148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'ice': 2, 'cream': 3, 'your': 4, 'my': 5, 'favourite': 6, 'is': 7, 'too': 8, 'dog': 9, 'chocolate': 10, 'food': 11, 'do': 12, 'you': 13, 'like': 14, 'likes': 15, 'flavour': 16, 'of': 17, \"isn't\": 18, 'good': 19, 'for': 20, 'dogs': 21, 'cat': 22, 'and': 23, 'parrot': 24, 'prefer': 25, 'brocolli': 26}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048e152",
   "metadata": {},
   "source": [
    "### Turn sentences to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdba5507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 6, 11, 7, 2, 3], [12, 13, 14, 2, 3, 8], [5, 9, 15, 2, 3, 8], [4, 6, 16, 17, 2, 3, 7, 10], [10, 18, 19, 20, 21], [4, 9, 4, 22, 23, 4, 24, 25, 26]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e796a1",
   "metadata": {},
   "source": [
    "### Make the sentences to be of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de064e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequence: \n",
      " [[ 0  0  0  5  6 11  7  2  3]\n",
      " [ 0  0  0 12 13 14  2  3  8]\n",
      " [ 0  0  0  5  9 15  2  3  8]\n",
      " [ 0  4  6 16 17  2  3  7 10]\n",
      " [ 0  0  0  0 10 18 19 20 21]\n",
      " [ 4  9  4 22 23  4 24 25 26]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "print(\"Padded sequence: \\n\",padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4bb5385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  5  6 11  7  2  3]\n",
      " [ 0  0  0  0  0  0  0  0  0 12 13 14  2  3  8]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  9 15  2  3  8]\n",
      " [ 0  0  0  0  0  0  0  4  6 16 17  2  3  7 10]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 10 18 19 20 21]\n",
      " [ 0  0  0  0  0  0  4  9  4 22 23  4 24 25 26]]\n"
     ]
    }
   ],
   "source": [
    "# Specify the maximum length\n",
    "padded = pad_sequences(sequences, maxlen = 15)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cda757f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  6 11  7  2  3  0  0  0  0  0  0  0  0  0]\n",
      " [12 13 14  2  3  8  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  9 15  2  3  8  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  6 16 17  2  3  7 10  0  0  0  0  0  0  0]\n",
      " [10 18 19 20 21  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  9  4 22 23  4 24 25 26  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding at the end of sentences\n",
    "padded = pad_sequences(sequences, maxlen = 15, padding = 'post')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "850917b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  2  3]\n",
      " [ 2  3  8]\n",
      " [ 2  3  8]\n",
      " [ 3  7 10]\n",
      " [19 20 21]\n",
      " [24 25 26]]\n"
     ]
    }
   ],
   "source": [
    "# Limit length\n",
    "padded = pad_sequences(sequences, maxlen = 3)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf76a5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
