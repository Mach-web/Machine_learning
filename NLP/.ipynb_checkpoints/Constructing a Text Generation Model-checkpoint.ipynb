{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8085551",
   "metadata": {},
   "source": [
    "### SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1213990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7fc66",
   "metadata": {},
   "source": [
    "### Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fffc7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist  song                        link  \\\n",
       "3   ABBA  Bang  /a/abba/bang_20598415.html   \n",
       "\n",
       "                                                text  \n",
       "3  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/login/Documents/songdata.csv'\n",
    "song_dataset = pd.read_csv(data_path, dtype = str)[:10]\n",
    "song_dataset.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef07512",
   "metadata": {},
   "source": [
    "### First 10 songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae9983",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af15b569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"look at her face, it's a wonderful face\", 'and it means something special to me', 'look at the way that she smiles when she sees me', 'how lucky can one fellow be?', \"she's just my kind of girl, she makes me feel fine\", 'who could ever believe that she could be mine?', \"she's just my kind of girl, without her i'm blue\", 'and if she ever leaves me what could i do, what could i do?', 'and when we go for a walk in the park', 'and she holds me and squeezes my hand', \"we'll go on walking for hours and talking\", 'about all the things that we plan', \"she's just my kind of girl, she makes me feel fine\", 'who could ever believe that she could be mine?', \"she's just my kind of girl, without her i'm blue\", 'and if she ever leaves me what could i do, what could i do?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3302/1288613871.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
      "/tmp/ipykernel_3302/1288613871.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[field] = dataset[field].str.lower()\n"
     ]
    }
   ],
   "source": [
    "def tokenize_corpus(corpus, num_words = -1):\n",
    "    if num_words > -1:\n",
    "        tokenizer = Tokenizer(num_words = num_words)\n",
    "    else:\n",
    "        tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    return tokenizer\n",
    "\n",
    "def create_lyrics_corpus(dataset, field):\n",
    "    dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
    "    dataset[field] = dataset[field].str.lower()\n",
    "    # make a long string to split by line\n",
    "    lyrics = dataset[field].str.cat()\n",
    "#     print(lyrics)\n",
    "    corpus = lyrics.split(\"\\n\")\n",
    "#     print(corpus)\n",
    "    # remove any trailing white spaces\n",
    "    for _ in range(len(corpus)):\n",
    "        corpus[_] = corpus[_].rstrip()\n",
    "    # remove empty lines\n",
    "    corpus = [_ for _ in corpus if _ != \"\"]\n",
    "    return corpus\n",
    "print(create_lyrics_corpus(song_dataset[:1], 'text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b68d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 1, 'a': 2, 'i': 3, 'and': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'be': 10, 'ma': 11, 'it': 12, 'of': 13, \"i'm\": 14, 'your': 15, 'love': 16, 'so': 17, 'as': 18, 'boomerang': 19, 'that': 20, 'in': 21, 'andante': 22, 'boom': 23, 'make': 24, 'dumb': 25, 'on': 26, 'oh': 27, 'dum': 28, 'for': 29, 'but': 30, 'new': 31, 'bang': 32, \"it's\": 33, 'like': 34, 'know': 35, 'now': 36, 'how': 37, 'could': 38, \"you're\": 39, 'sing': 40, 'never': 41, 'no': 42, 'hum': 43, 'chiquitita': 44, 'can': 45, 'we': 46, 'song': 47, 'had': 48, 'good': 49, \"you'll\": 50, 'she': 51, 'just': 52, 'girl': 53, 'again': 54, 'will': 55, 'take': 56, 'please': 57, 'let': 58, 'am': 59, 'eyes': 60, 'was': 61, 'always': 62, 'cassandra': 63, 'blue': 64, 'time': 65, \"don't\": 66, 'were': 67, 'return': 68, 'once': 69, 'then': 70, 'sorry': 71, \"cryin'\": 72, 'over': 73, 'feel': 74, 'ever': 75, 'believe': 76, 'what': 77, 'do': 78, 'go': 79, 'all': 80, 'out': 81, 'think': 82, 'every': 83, 'leave': 84, 'look': 85, 'at': 86, 'way': 87, 'one': 88, 'music': 89, 'down': 90, 'our': 91, 'give': 92, 'learn': 93, 'more': 94, 'us': 95, 'would': 96, 'there': 97, 'before': 98, 'when': 99, 'with': 100, 'feeling': 101, 'play': 102, \"'cause\": 103, 'away': 104, 'here': 105, 'have': 106, 'yes': 107, 'baby': 108, 'get': 109, \"didn't\": 110, 'see': 111, 'did': 112, 'closed': 113, 'realized': 114, 'crazy': 115, 'world': 116, 'lord': 117, \"she's\": 118, 'kind': 119, 'without': 120, 'if': 121, 'touch': 122, 'strong': 123, 'making': 124, 'such': 125, 'found': 126, 'true': 127, 'stay': 128, 'together': 129, 'thought': 130, 'come': 131, 'they': 132, 'sweet': 133, 'tender': 134, 'sender': 135, 'tune': 136, 'de': 137, 'gonna': 138, 'last': 139, 'leaving': 140, 'sleep': 141, 'only': 142, 'saw': 143, 'tell': 144, \"he's\": 145, 'her': 146, 'sound': 147, 'tread': 148, 'lightly': 149, 'ground': 150, \"i'll\": 151, 'show': 152, 'life': 153, 'too': 154, 'used': 155, 'darling': 156, 'meant': 157, 'break': 158, 'end': 159, 'yourself': 160, 'little': 161, \"you've\": 162, 'by': 163, \"they're\": 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'about': 207, 'things': 208, 'slow': 209, \"there's\": 210, 'talk': 211, 'why': 212, 'up': 213, 'lousy': 214, 'packing': 215, \"i've\": 216, 'gotta': 217, 'near': 218, 'keeping': 219, 'intention': 220, 'growing': 221, 'taking': 222, 'dimension': 223, 'even': 224, 'better': 225, 'thank': 226, 'god': 227, 'not': 228, 'somebody': 229, 'happy': 230, 'question': 231, 'smile': 232, 'mean': 233, 'much': 234, 'kisses': 235, 'around': 236, 'anywhere': 237, 'advice': 238, 'care': 239, 'use': 240, 'selfish': 241, 'tool': 242, 'fool': 243, 'showing': 244, 'throwing': 245, 'warm': 246, 'kiss': 247, 'surrender': 248, 'giving': 249, 'been': 250, 'door': 251, 'burning': 252, 'bridges': 253, 'being': 254, 'moving': 255, 'though': 256, 'behind': 257, 'are': 258, 'must': 259, 'sure': 260, 'stood': 261, 'hope': 262, 'this': 263, 'deny': 264, 'sad': 265, 'quiet': 266, 'truth': 267, 'heartaches': 268, 'scars': 269, 'dancing': 270, 'sky': 271, 'shining': 272, 'above': 273, 'hear': 274, 'came': 275, \"couldn't\": 276, 'everything': 277, 'back': 278, 'long': 279, \"waitin'\": 280, 'cold': 281, 'chills': 282, 'bone': 283, \"you'd\": 284, 'wonderful': 285, 'means': 286, 'special': 287, 'smiles': 288, 'lucky': 289, 'fellow': 290, 'park': 291, 'holds': 292, 'squeezes': 293, \"we'll\": 294, 'walking': 295, 'hours': 296, 'talking': 297, 'plan': 298, 'easy': 299, 'gently': 300, 'summer': 301, 'evening': 302, 'breeze': 303, 'grow': 304, 'fingers': 305, 'soft': 306, 'light': 307, 'body': 308, 'velvet': 309, 'night': 310, 'soul': 311, 'slowly': 312, 'shimmer': 313, 'thousand': 314, 'butterflies': 315, 'float': 316, 'put': 317, 'rotten': 318, 'boy': 319, 'tough': 320, 'stuff': 321, 'saying': 322, 'need': 323, 'anymore': 324, 'enough': 325, 'standing': 326, 'creep': 327, 'felt': 328, 'cheap': 329, 'notion': 330, 'deep': 331, 'mistake': 332, 'entitled': 333, 'another': 334, 'beg': 335, 'forgive': 336, 'an': 337, 'feels': 338, 'well': 339, 'hoot': 340, 'holler': 341, 'mad': 342, 'under': 343, 'heel': 344, 'holy': 345, 'christ': 346, 'deal': 347, 'sick': 348, 'tired': 349, 'tedious': 350, 'ways': 351, \"ain't\": 352, \"walkin'\": 353, 'cutting': 354, 'tie': 355, 'wanna': 356, 'into': 357, 'eye': 358, 'myself': 359, 'counting': 360, 'pride': 361, 'un': 362, 'right': 363, \"neighbour's\": 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, \"what's\": 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, \"love's\": 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, \"i'd\": 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, \"life's\": 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, \"sittin'\": 493, 'memories': 494}\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "corpus = create_lyrics_corpus(song_dataset, 'text')\n",
    "tokenizer = tokenize_corpus(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca85eb",
   "metadata": {},
   "source": [
    "### Create sequences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0598a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "101\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0  85  86 146 197  33\n",
      "   2]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0  85  86 146 197  33   2\n",
      " 285]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "        \n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
    "\n",
    "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
    "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes = total_words)\n",
    "    \n",
    "print(tokenizer.word_index['know'])   \n",
    "print(tokenizer.word_index['feeling'])\n",
    "\n",
    "print(input_sequences[5])\n",
    "print(input_sequences[6])\n",
    "\n",
    "print(one_hot_labels[5])\n",
    "print(one_hot_labels[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ebafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
