{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236b3773",
   "metadata": {},
   "source": [
    "## Preparing text to use with tensorflow models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3750c0",
   "metadata": {},
   "source": [
    "### Import classes you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "185be88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730af3e",
   "metadata": {},
   "source": [
    "### Write some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "774ad0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My favourite food is ice cream', 'do you like ice cream too?', 'My dog likes ice cream too!', 'your favourite flavour of ice cream is chocolate', \"chocolate isn't good for dogs\", 'your dog your cat, and your parrot prefer brocolli']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"My favourite food is ice cream\",\n",
    "    \"do you like ice cream too?\",\n",
    "    \"My dog likes ice cream too!\",\n",
    "    \"your favourite flavour of ice cream is chocolate\",\n",
    "    \"chocolate isn't good for dogs\",\n",
    "    \"your dog your cat, and your parrot prefer brocolli\"\n",
    "]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a79eb",
   "metadata": {},
   "source": [
    "### Create a tokenizer and define an out of vocabulary token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "163b636e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.text.Tokenizer at 0x7f6f6e10fb50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1d1b1",
   "metadata": {},
   "source": [
    "### Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "048393cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'ice': 2, 'cream': 3, 'your': 4, 'my': 5, 'favourite': 6, 'is': 7, 'too': 8, 'dog': 9, 'chocolate': 10, 'food': 11, 'do': 12, 'you': 13, 'like': 14, 'likes': 15, 'flavour': 16, 'of': 17, \"isn't\": 18, 'good': 19, 'for': 20, 'dogs': 21, 'cat': 22, 'and': 23, 'parrot': 24, 'prefer': 25, 'brocolli': 26}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9299c2c",
   "metadata": {},
   "source": [
    "### Turn sentences to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88f9eafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 6, 11, 7, 2, 3], [12, 13, 14, 2, 3, 8], [5, 9, 15, 2, 3, 8], [4, 6, 16, 17, 2, 3, 7, 10], [10, 18, 19, 20, 21], [4, 9, 4, 22, 23, 4, 24, 25, 26]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f50845",
   "metadata": {},
   "source": [
    "### Make the sentences to be of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff09fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequence: \n",
      " [[ 0  0  0  5  6 11  7  2  3]\n",
      " [ 0  0  0 12 13 14  2  3  8]\n",
      " [ 0  0  0  5  9 15  2  3  8]\n",
      " [ 0  4  6 16 17  2  3  7 10]\n",
      " [ 0  0  0  0 10 18 19 20 21]\n",
      " [ 4  9  4 22 23  4 24 25 26]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "print(\"Padded sequence: \\n\",padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86c16bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  5  6 11  7  2  3]\n",
      " [ 0  0  0  0  0  0  0  0  0 12 13 14  2  3  8]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  9 15  2  3  8]\n",
      " [ 0  0  0  0  0  0  0  4  6 16 17  2  3  7 10]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 10 18 19 20 21]\n",
      " [ 0  0  0  0  0  0  4  9  4 22 23  4 24 25 26]]\n"
     ]
    }
   ],
   "source": [
    "# Specify the maximum length\n",
    "padded = pad_sequences(sequences, maxlen = 15)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "488a103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  6 11  7  2  3  0  0  0  0  0  0  0  0  0]\n",
      " [12 13 14  2  3  8  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  9 15  2  3  8  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  6 16 17  2  3  7 10  0  0  0  0  0  0  0]\n",
      " [10 18 19 20 21  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  9  4 22 23  4 24 25 26  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding at the end of sentences\n",
    "padded = pad_sequences(sequences, maxlen = 15, padding = 'post')\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82310a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  2  3]\n",
      " [ 2  3  8]\n",
      " [ 2  3  8]\n",
      " [ 3  7 10]\n",
      " [19 20 21]\n",
      " [24 25 26]]\n"
     ]
    }
   ],
   "source": [
    "# Limit length\n",
    "padded = pad_sequences(sequences, maxlen = 3)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9bf4d",
   "metadata": {},
   "source": [
    "##### What happens if some of the sentences contain words that are not in the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4cb85cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"my best friend's favourite ice cream flavour is strawberry\", \"my dog's best friend is a manatee\"]\n",
      "\n",
      "<OOV> has the number  1  in the word index\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    \"my best friend's favourite ice cream flavour is strawberry\",\n",
    "    \"my dog's best friend is a manatee\"\n",
    "]\n",
    "print(test_data, end = \"\\n\\n\")\n",
    "print(\"<OOV> has the number \", word_index['<OOV>'], \" in the word index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c4c896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sequence: \n",
      " [[5, 1, 1, 6, 2, 3, 16, 7, 1], [5, 1, 1, 1, 7, 1, 1]]\n",
      "\n",
      " Padded test sequences: \n",
      " [[ 0  5  1  1  6  2  3 16  7  1]\n",
      " [ 0  0  0  5  1  1  1  7  1  1]]\n"
     ]
    }
   ],
   "source": [
    "# convert the test_data to sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"Test sequence: \\n\", test_sequences)\n",
    "# pad test sequences\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen = 10)\n",
    "print('\\n Padded test sequences: \\n', padded_test_sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
