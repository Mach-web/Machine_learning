{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8085551",
   "metadata": {},
   "source": [
    "### SetUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1213990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 18:58:54.441206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Embedding\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7fc66",
   "metadata": {},
   "source": [
    "### Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fffc7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist              song                                   link  \\\n",
       "1   ABBA  Andante, Andante  /a/abba/andante+andante_20002708.html   \n",
       "\n",
       "                                                text  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/login/Documents/songdata.csv'\n",
    "song_dataset = pd.read_csv(data_path, dtype = str)[:10]\n",
    "song_dataset.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef07512",
   "metadata": {},
   "source": [
    "### First 10 songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae9983",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "872a2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "<pandas.core.strings.accessor.StringMethods object at 0x7f592c288dd0>\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)\n",
    "x = song_dataset['text'].str\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af15b569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"look at her face, it's a wonderful face\", 'and it means something special to me', 'look at the way that she smiles when she sees me', 'how lucky can one fellow be?', \"she's just my kind of girl, she makes me feel fine\", 'who could ever believe that she could be mine?', \"she's just my kind of girl, without her i'm blue\", 'and if she ever leaves me what could i do, what could i do?', 'and when we go for a walk in the park', 'and she holds me and squeezes my hand', \"we'll go on walking for hours and talking\", 'about all the things that we plan', \"she's just my kind of girl, she makes me feel fine\", 'who could ever believe that she could be mine?', \"she's just my kind of girl, without her i'm blue\", 'and if she ever leaves me what could i do, what could i do?', 'take it easy with me, please', 'touch me gently like a summer evening breeze', 'take your time, make it slow', 'andante, andante', 'just let the feeling grow', 'make your fingers soft and light', 'let your body be the velvet of the night', 'touch my soul, you know how', 'andante, andante', 'go slowly with me now', \"i'm your music\", '(i am your music and i am your song)', \"i'm your song\", '(i am your music and i am your song)', 'play me time and time again and make me strong', \"(play me again 'cause you're making me strong)\", 'make me sing, make me sound', '(you make me sing and you make me)', 'andante, andante', 'tread lightly on my ground', 'andante, andante', \"oh please don't let me down\", \"there's a shimmer in your eyes\", 'like the feeling of a thousand butterflies', \"please don't talk, go on, play\", 'andante, andante', 'and let me float away', \"i'm your music\", '(i am your music and i am your song)', \"i'm your song\", '(i am your music and i am your song)', 'play me time and time again and make me strong', \"(play me again 'cause you're making me strong)\", 'make me sing, make me sound', '(you make me sing and you make me)', 'andante, andante', 'tread lightly on my ground', 'andante, andante', \"oh please don't let me down\", 'make me sing, make me sound', '(you make me sing and you make me)', 'andante, andante', 'tread lightly on my ground', 'andante, andante', \"oh please don't let me down\", 'andante, andante', \"oh please don't let me down\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4542/487909910.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
      "/tmp/ipykernel_4542/487909910.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[field] = dataset[field].str.lower()\n"
     ]
    }
   ],
   "source": [
    "def tokenize_corpus(corpus, num_words = -1):\n",
    "    if num_words > -1:\n",
    "        tokenizer = Tokenizer(num_words = num_words)\n",
    "    else:\n",
    "        tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    return tokenizer\n",
    "\n",
    "def create_lyrics_corpus(dataset, field):\n",
    "    dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
    "    dataset[field] = dataset[field].str.lower()\n",
    "    # make a long string to split by line\n",
    "    lyrics = dataset[field].str.cat()\n",
    "#     print(lyrics)\n",
    "    corpus = lyrics.split(\"\\n\")\n",
    "#     print(corpus)\n",
    "    # remove any trailing white spaces\n",
    "    for _ in range(len(corpus)):\n",
    "        corpus[_] = corpus[_].rstrip()\n",
    "    # remove empty lines\n",
    "    corpus = [_ for _ in corpus if _ != \"\"]\n",
    "    return corpus\n",
    "print(create_lyrics_corpus(song_dataset[:2], 'text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32b68d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 1, 'a': 2, 'i': 3, 'and': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'be': 10, 'ma': 11, 'it': 12, 'of': 13, \"i'm\": 14, 'your': 15, 'love': 16, 'so': 17, 'as': 18, 'boomerang': 19, 'that': 20, 'in': 21, 'andante': 22, 'boom': 23, 'make': 24, 'dumb': 25, 'on': 26, 'oh': 27, 'dum': 28, 'for': 29, 'but': 30, 'new': 31, 'bang': 32, \"it's\": 33, 'like': 34, 'know': 35, 'now': 36, 'how': 37, 'could': 38, \"you're\": 39, 'sing': 40, 'never': 41, 'no': 42, 'hum': 43, 'chiquitita': 44, 'can': 45, 'we': 46, 'song': 47, 'had': 48, 'good': 49, \"you'll\": 50, 'she': 51, 'just': 52, 'girl': 53, 'again': 54, 'will': 55, 'take': 56, 'please': 57, 'let': 58, 'am': 59, 'eyes': 60, 'was': 61, 'always': 62, 'cassandra': 63, 'blue': 64, 'time': 65, \"don't\": 66, 'were': 67, 'return': 68, 'once': 69, 'then': 70, 'sorry': 71, \"cryin'\": 72, 'over': 73, 'feel': 74, 'ever': 75, 'believe': 76, 'what': 77, 'do': 78, 'go': 79, 'all': 80, 'out': 81, 'think': 82, 'every': 83, 'leave': 84, 'look': 85, 'at': 86, 'way': 87, 'one': 88, 'music': 89, 'down': 90, 'our': 91, 'give': 92, 'learn': 93, 'more': 94, 'us': 95, 'would': 96, 'there': 97, 'before': 98, 'when': 99, 'with': 100, 'feeling': 101, 'play': 102, \"'cause\": 103, 'away': 104, 'here': 105, 'have': 106, 'yes': 107, 'baby': 108, 'get': 109, \"didn't\": 110, 'see': 111, 'did': 112, 'closed': 113, 'realized': 114, 'crazy': 115, 'world': 116, 'lord': 117, \"she's\": 118, 'kind': 119, 'without': 120, 'if': 121, 'touch': 122, 'strong': 123, 'making': 124, 'such': 125, 'found': 126, 'true': 127, 'stay': 128, 'together': 129, 'thought': 130, 'come': 131, 'they': 132, 'sweet': 133, 'tender': 134, 'sender': 135, 'tune': 136, 'de': 137, 'gonna': 138, 'last': 139, 'leaving': 140, 'sleep': 141, 'only': 142, 'saw': 143, 'tell': 144, \"he's\": 145, 'her': 146, 'sound': 147, 'tread': 148, 'lightly': 149, 'ground': 150, \"i'll\": 151, 'show': 152, 'life': 153, 'too': 154, 'used': 155, 'darling': 156, 'meant': 157, 'break': 158, 'end': 159, 'yourself': 160, 'little': 161, \"you've\": 162, 'by': 163, \"they're\": 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'about': 207, 'things': 208, 'slow': 209, \"there's\": 210, 'talk': 211, 'why': 212, 'up': 213, 'lousy': 214, 'packing': 215, \"i've\": 216, 'gotta': 217, 'near': 218, 'keeping': 219, 'intention': 220, 'growing': 221, 'taking': 222, 'dimension': 223, 'even': 224, 'better': 225, 'thank': 226, 'god': 227, 'not': 228, 'somebody': 229, 'happy': 230, 'question': 231, 'smile': 232, 'mean': 233, 'much': 234, 'kisses': 235, 'around': 236, 'anywhere': 237, 'advice': 238, 'care': 239, 'use': 240, 'selfish': 241, 'tool': 242, 'fool': 243, 'showing': 244, 'throwing': 245, 'warm': 246, 'kiss': 247, 'surrender': 248, 'giving': 249, 'been': 250, 'door': 251, 'burning': 252, 'bridges': 253, 'being': 254, 'moving': 255, 'though': 256, 'behind': 257, 'are': 258, 'must': 259, 'sure': 260, 'stood': 261, 'hope': 262, 'this': 263, 'deny': 264, 'sad': 265, 'quiet': 266, 'truth': 267, 'heartaches': 268, 'scars': 269, 'dancing': 270, 'sky': 271, 'shining': 272, 'above': 273, 'hear': 274, 'came': 275, \"couldn't\": 276, 'everything': 277, 'back': 278, 'long': 279, \"waitin'\": 280, 'cold': 281, 'chills': 282, 'bone': 283, \"you'd\": 284, 'wonderful': 285, 'means': 286, 'special': 287, 'smiles': 288, 'lucky': 289, 'fellow': 290, 'park': 291, 'holds': 292, 'squeezes': 293, \"we'll\": 294, 'walking': 295, 'hours': 296, 'talking': 297, 'plan': 298, 'easy': 299, 'gently': 300, 'summer': 301, 'evening': 302, 'breeze': 303, 'grow': 304, 'fingers': 305, 'soft': 306, 'light': 307, 'body': 308, 'velvet': 309, 'night': 310, 'soul': 311, 'slowly': 312, 'shimmer': 313, 'thousand': 314, 'butterflies': 315, 'float': 316, 'put': 317, 'rotten': 318, 'boy': 319, 'tough': 320, 'stuff': 321, 'saying': 322, 'need': 323, 'anymore': 324, 'enough': 325, 'standing': 326, 'creep': 327, 'felt': 328, 'cheap': 329, 'notion': 330, 'deep': 331, 'mistake': 332, 'entitled': 333, 'another': 334, 'beg': 335, 'forgive': 336, 'an': 337, 'feels': 338, 'well': 339, 'hoot': 340, 'holler': 341, 'mad': 342, 'under': 343, 'heel': 344, 'holy': 345, 'christ': 346, 'deal': 347, 'sick': 348, 'tired': 349, 'tedious': 350, 'ways': 351, \"ain't\": 352, \"walkin'\": 353, 'cutting': 354, 'tie': 355, 'wanna': 356, 'into': 357, 'eye': 358, 'myself': 359, 'counting': 360, 'pride': 361, 'un': 362, 'right': 363, \"neighbour's\": 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, \"what's\": 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, \"love's\": 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, \"i'd\": 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, \"life's\": 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, \"sittin'\": 493, 'memories': 494}\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "corpus = create_lyrics_corpus(song_dataset, 'text')\n",
    "tokenizer = tokenize_corpus(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a792d",
   "metadata": {},
   "source": [
    "### Create sequences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66429236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85, 86, 146, 197, 33, 2, 285, 197]]\n",
      "[85, 86, 146, 197, 33, 2, 285, 197]\n",
      "[85]\n",
      "[85, 86]\n",
      "[85, 86, 146]\n",
      "[85, 86, 146, 197]\n",
      "[85, 86, 146, 197, 33]\n",
      "[85, 86, 146, 197, 33, 2]\n",
      "[85, 86, 146, 197, 33, 2, 285]\n",
      "[85, 86, 146, 197, 33, 2, 285, 197]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences([corpus[0]]))\n",
    "tok_list = tokenizer.texts_to_sequences([corpus[0]])[0]\n",
    "print(tok_list)\n",
    "for i in range(len(tok_list)):\n",
    "    n_gram_seq = tok_list[:i+1]\n",
    "    print(n_gram_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdbdca78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "101\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0  85  86 146 197  33\n",
      "   2]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0  85  86 146 197  33   2\n",
      " 285]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "        \n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
    "\n",
    "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
    "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes = total_words)\n",
    "    \n",
    "print(tokenizer.word_index['know'])   \n",
    "print(tokenizer.word_index['feeling'])\n",
    "\n",
    "print(input_sequences[5])\n",
    "print(input_sequences[6])\n",
    "\n",
    "print(one_hot_labels[5])\n",
    "print(one_hot_labels[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928cd01d",
   "metadata": {},
   "source": [
    "### Train a Text Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ece4950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "65/65 [==============================] - 6s 14ms/step - loss: 5.9711 - accuracy: 0.0274\n",
      "Epoch 2/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 5.4088 - accuracy: 0.0404\n",
      "Epoch 3/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 5.3269 - accuracy: 0.0486\n",
      "Epoch 4/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 5.2615 - accuracy: 0.0486\n",
      "Epoch 5/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 5.1785 - accuracy: 0.0510\n",
      "Epoch 6/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 5.0946 - accuracy: 0.0530\n",
      "Epoch 7/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 5.0158 - accuracy: 0.0626\n",
      "Epoch 8/200\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 4.9320 - accuracy: 0.0737\n",
      "Epoch 9/200\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 4.8366 - accuracy: 0.0828\n",
      "Epoch 10/200\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 4.7311 - accuracy: 0.0939\n",
      "Epoch 11/200\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 4.6238 - accuracy: 0.0992\n",
      "Epoch 12/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 4.5141 - accuracy: 0.1151\n",
      "Epoch 13/200\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 4.4069 - accuracy: 0.1295\n",
      "Epoch 14/200\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 4.3053 - accuracy: 0.1401\n",
      "Epoch 15/200\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 4.1977 - accuracy: 0.1483\n",
      "Epoch 16/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 4.1008 - accuracy: 0.1714\n",
      "Epoch 17/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 4.0110 - accuracy: 0.1854\n",
      "Epoch 18/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 3.9104 - accuracy: 0.2070\n",
      "Epoch 19/200\n",
      "65/65 [==============================] - 1s 21ms/step - loss: 3.8318 - accuracy: 0.2167\n",
      "Epoch 20/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 3.7604 - accuracy: 0.2378\n",
      "Epoch 21/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 3.6689 - accuracy: 0.2581\n",
      "Epoch 22/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 3.5814 - accuracy: 0.2720\n",
      "Epoch 23/200\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 3.5051 - accuracy: 0.2908\n",
      "Epoch 24/200\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 3.4271 - accuracy: 0.3130\n",
      "Epoch 25/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 3.3567 - accuracy: 0.3226\n",
      "Epoch 26/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 3.2880 - accuracy: 0.3284\n",
      "Epoch 27/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 3.2179 - accuracy: 0.3457\n",
      "Epoch 28/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 3.1510 - accuracy: 0.3664\n",
      "Epoch 29/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 3.0817 - accuracy: 0.3741\n",
      "Epoch 30/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 3.0032 - accuracy: 0.3934\n",
      "Epoch 31/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 2.9386 - accuracy: 0.3987\n",
      "Epoch 32/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 2.8741 - accuracy: 0.4280\n",
      "Epoch 33/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 2.8132 - accuracy: 0.4309\n",
      "Epoch 34/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 2.7536 - accuracy: 0.4439\n",
      "Epoch 35/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 2.6956 - accuracy: 0.4531\n",
      "Epoch 36/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 2.6411 - accuracy: 0.4704\n",
      "Epoch 37/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 2.5815 - accuracy: 0.4776\n",
      "Epoch 38/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 2.5264 - accuracy: 0.4930\n",
      "Epoch 39/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 2.4707 - accuracy: 0.5031\n",
      "Epoch 40/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 2.4343 - accuracy: 0.5084\n",
      "Epoch 41/200\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 2.3926 - accuracy: 0.5190\n",
      "Epoch 42/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 2.3377 - accuracy: 0.5282\n",
      "Epoch 43/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 2.2835 - accuracy: 0.5364\n",
      "Epoch 44/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 2.2453 - accuracy: 0.5421\n",
      "Epoch 45/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 2.2010 - accuracy: 0.5532\n",
      "Epoch 46/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 2.1592 - accuracy: 0.5619\n",
      "Epoch 47/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 2.1152 - accuracy: 0.5696\n",
      "Epoch 48/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 2.0623 - accuracy: 0.5778\n",
      "Epoch 49/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 2.0253 - accuracy: 0.5826\n",
      "Epoch 50/200\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 2.0069 - accuracy: 0.5855\n",
      "Epoch 51/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 1.9791 - accuracy: 0.5908\n",
      "Epoch 52/200\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 1.9449 - accuracy: 0.5985\n",
      "Epoch 53/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.8923 - accuracy: 0.6095\n",
      "Epoch 54/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 1.8929 - accuracy: 0.6071\n",
      "Epoch 55/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 1.8663 - accuracy: 0.6047\n",
      "Epoch 56/200\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 1.8018 - accuracy: 0.6264\n",
      "Epoch 57/200\n",
      "65/65 [==============================] - 1s 21ms/step - loss: 1.7754 - accuracy: 0.6273\n",
      "Epoch 58/200\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 1.7551 - accuracy: 0.6403\n",
      "Epoch 59/200\n",
      "65/65 [==============================] - 1s 20ms/step - loss: 1.7114 - accuracy: 0.6413\n",
      "Epoch 60/200\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 1.7155 - accuracy: 0.6485\n",
      "Epoch 61/200\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 1.6619 - accuracy: 0.6596\n",
      "Epoch 62/200\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 1.6260 - accuracy: 0.6649\n",
      "Epoch 63/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 1.5955 - accuracy: 0.6740\n",
      "Epoch 64/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.5768 - accuracy: 0.6750\n",
      "Epoch 65/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.5402 - accuracy: 0.6866\n",
      "Epoch 66/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.5088 - accuracy: 0.6972\n",
      "Epoch 67/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.4951 - accuracy: 0.6962\n",
      "Epoch 68/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 1.4623 - accuracy: 0.6938\n",
      "Epoch 69/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 1.4438 - accuracy: 0.7078\n",
      "Epoch 70/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 1.4121 - accuracy: 0.7111\n",
      "Epoch 71/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.4227 - accuracy: 0.7111\n",
      "Epoch 72/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.4115 - accuracy: 0.7044\n",
      "Epoch 73/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.3653 - accuracy: 0.7217\n",
      "Epoch 74/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.3332 - accuracy: 0.7280\n",
      "Epoch 75/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 1.3157 - accuracy: 0.7275\n",
      "Epoch 76/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 1.2957 - accuracy: 0.7347\n",
      "Epoch 77/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.2786 - accuracy: 0.7405\n",
      "Epoch 78/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 1.2505 - accuracy: 0.7458\n",
      "Epoch 79/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.2388 - accuracy: 0.7390\n",
      "Epoch 80/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.2210 - accuracy: 0.7535\n",
      "Epoch 81/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.2393 - accuracy: 0.7366\n",
      "Epoch 82/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.1896 - accuracy: 0.7559\n",
      "Epoch 83/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 1.1666 - accuracy: 0.7607\n",
      "Epoch 84/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 1.1804 - accuracy: 0.7487\n",
      "Epoch 85/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 1.1433 - accuracy: 0.7636\n",
      "Epoch 86/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.1119 - accuracy: 0.7684\n",
      "Epoch 87/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.1257 - accuracy: 0.7631\n",
      "Epoch 88/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.0901 - accuracy: 0.7814\n",
      "Epoch 89/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 1.0749 - accuracy: 0.7771\n",
      "Epoch 90/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 1.0671 - accuracy: 0.7795\n",
      "Epoch 91/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.0412 - accuracy: 0.7829\n",
      "Epoch 92/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 1.0191 - accuracy: 0.7891\n",
      "Epoch 93/200\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 1.0054 - accuracy: 0.7983\n",
      "Epoch 94/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.9991 - accuracy: 0.7949\n",
      "Epoch 95/200\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.9786 - accuracy: 0.8016\n",
      "Epoch 96/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.9616 - accuracy: 0.8050\n",
      "Epoch 97/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.9517 - accuracy: 0.8031\n",
      "Epoch 98/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.9334 - accuracy: 0.8045\n",
      "Epoch 99/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.9278 - accuracy: 0.8045\n",
      "Epoch 100/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.9079 - accuracy: 0.8142\n",
      "Epoch 101/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.8979 - accuracy: 0.8137\n",
      "Epoch 102/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 0.8969 - accuracy: 0.8050\n",
      "Epoch 103/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.8735 - accuracy: 0.8151\n",
      "Epoch 104/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.8618 - accuracy: 0.8185\n",
      "Epoch 105/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.8490 - accuracy: 0.8233\n",
      "Epoch 106/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.8405 - accuracy: 0.8219\n",
      "Epoch 107/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.8212 - accuracy: 0.8276\n",
      "Epoch 108/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.8107 - accuracy: 0.8281\n",
      "Epoch 109/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.8026 - accuracy: 0.8325\n",
      "Epoch 110/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7968 - accuracy: 0.8320\n",
      "Epoch 111/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7856 - accuracy: 0.8349\n",
      "Epoch 112/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7725 - accuracy: 0.8334\n",
      "Epoch 113/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7690 - accuracy: 0.8373\n",
      "Epoch 114/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7722 - accuracy: 0.8325\n",
      "Epoch 115/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7517 - accuracy: 0.8411\n",
      "Epoch 116/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7491 - accuracy: 0.8377\n",
      "Epoch 117/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7328 - accuracy: 0.8397\n",
      "Epoch 118/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7356 - accuracy: 0.8387\n",
      "Epoch 119/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7473 - accuracy: 0.8349\n",
      "Epoch 120/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7576 - accuracy: 0.8320\n",
      "Epoch 121/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7329 - accuracy: 0.8325\n",
      "Epoch 122/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7073 - accuracy: 0.8440\n",
      "Epoch 123/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.7115 - accuracy: 0.8469\n",
      "Epoch 124/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6950 - accuracy: 0.8503\n",
      "Epoch 125/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6841 - accuracy: 0.8450\n",
      "Epoch 126/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6736 - accuracy: 0.8527\n",
      "Epoch 127/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6686 - accuracy: 0.8532\n",
      "Epoch 128/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6493 - accuracy: 0.8556\n",
      "Epoch 129/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6412 - accuracy: 0.8541\n",
      "Epoch 130/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6326 - accuracy: 0.8628\n",
      "Epoch 131/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6253 - accuracy: 0.8628\n",
      "Epoch 132/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6164 - accuracy: 0.8666\n",
      "Epoch 133/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6087 - accuracy: 0.8686\n",
      "Epoch 134/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.6018 - accuracy: 0.8652\n",
      "Epoch 135/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5976 - accuracy: 0.8671\n",
      "Epoch 136/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5909 - accuracy: 0.8666\n",
      "Epoch 137/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5897 - accuracy: 0.8652\n",
      "Epoch 138/200\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 0.5830 - accuracy: 0.8690\n",
      "Epoch 139/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.5860 - accuracy: 0.8662\n",
      "Epoch 140/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5759 - accuracy: 0.8710\n",
      "Epoch 141/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5654 - accuracy: 0.8748\n",
      "Epoch 142/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5559 - accuracy: 0.8710\n",
      "Epoch 143/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5636 - accuracy: 0.8734\n",
      "Epoch 144/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5551 - accuracy: 0.8782\n",
      "Epoch 145/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5535 - accuracy: 0.8700\n",
      "Epoch 146/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5514 - accuracy: 0.8729\n",
      "Epoch 147/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5403 - accuracy: 0.8739\n",
      "Epoch 148/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.5396 - accuracy: 0.8763\n",
      "Epoch 149/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.5293 - accuracy: 0.8767\n",
      "Epoch 150/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5164 - accuracy: 0.8835\n",
      "Epoch 151/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5143 - accuracy: 0.8772\n",
      "Epoch 152/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.5024 - accuracy: 0.8801\n",
      "Epoch 153/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4980 - accuracy: 0.8825\n",
      "Epoch 154/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4936 - accuracy: 0.8816\n",
      "Epoch 155/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4947 - accuracy: 0.8825\n",
      "Epoch 156/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.5066 - accuracy: 0.8758\n",
      "Epoch 157/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4977 - accuracy: 0.8844\n",
      "Epoch 158/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4831 - accuracy: 0.8830\n",
      "Epoch 159/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4866 - accuracy: 0.8859\n",
      "Epoch 160/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4777 - accuracy: 0.8840\n",
      "Epoch 161/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4686 - accuracy: 0.8830\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4670 - accuracy: 0.8840\n",
      "Epoch 163/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4650 - accuracy: 0.8806\n",
      "Epoch 164/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4779 - accuracy: 0.8825\n",
      "Epoch 165/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4700 - accuracy: 0.8835\n",
      "Epoch 166/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4531 - accuracy: 0.8869\n",
      "Epoch 167/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4766 - accuracy: 0.8758\n",
      "Epoch 168/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4670 - accuracy: 0.8796\n",
      "Epoch 169/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4495 - accuracy: 0.8878\n",
      "Epoch 170/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4555 - accuracy: 0.8859\n",
      "Epoch 171/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4429 - accuracy: 0.8854\n",
      "Epoch 172/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4288 - accuracy: 0.8888\n",
      "Epoch 173/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4218 - accuracy: 0.8883\n",
      "Epoch 174/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4183 - accuracy: 0.8917\n",
      "Epoch 175/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4148 - accuracy: 0.8893\n",
      "Epoch 176/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4106 - accuracy: 0.8912\n",
      "Epoch 177/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4065 - accuracy: 0.8941\n",
      "Epoch 178/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4031 - accuracy: 0.8960\n",
      "Epoch 179/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.3990 - accuracy: 0.8922\n",
      "Epoch 180/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4076 - accuracy: 0.8912\n",
      "Epoch 181/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4137 - accuracy: 0.8960\n",
      "Epoch 182/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4022 - accuracy: 0.8859\n",
      "Epoch 183/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.3987 - accuracy: 0.8946\n",
      "Epoch 184/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4263 - accuracy: 0.8830\n",
      "Epoch 185/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.4085 - accuracy: 0.8917\n",
      "Epoch 186/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.4031 - accuracy: 0.8922\n",
      "Epoch 187/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.3860 - accuracy: 0.8950\n",
      "Epoch 188/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.3898 - accuracy: 0.8922\n",
      "Epoch 189/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.3841 - accuracy: 0.8931\n",
      "Epoch 190/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.3738 - accuracy: 0.8965\n",
      "Epoch 191/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.3724 - accuracy: 0.8931\n",
      "Epoch 192/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.3653 - accuracy: 0.8984\n",
      "Epoch 193/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.3627 - accuracy: 0.8984\n",
      "Epoch 194/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.3585 - accuracy: 0.8994\n",
      "Epoch 195/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.3557 - accuracy: 0.9037\n",
      "Epoch 196/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.3595 - accuracy: 0.8955\n",
      "Epoch 197/200\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 0.3540 - accuracy: 0.9008\n",
      "Epoch 198/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.3465 - accuracy: 0.9047\n",
      "Epoch 199/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.3462 - accuracy: 0.9027\n",
      "Epoch 200/200\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.3446 - accuracy: 0.8989\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length = max_sequence_len - 1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation = \"softmax\"))\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n",
    "history = model.fit(input_sequences, one_hot_labels, epochs = 200, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27510887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
