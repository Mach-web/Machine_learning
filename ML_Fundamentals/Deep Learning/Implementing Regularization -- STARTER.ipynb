{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5225ae9",
   "metadata": {},
   "source": [
    "# Implementing Regularization\n",
    "In this exercise, you will implement both L1 and L2 regularization from scratch in NumPy. \n",
    "In PyTorch, L2 regularization is typically handled in the optimizer, via the `weight_decay` parameter, but we will also implement a manual L1 and L2 loss penalty in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e67fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb93ba8",
   "metadata": {},
   "source": [
    "## L1 Regularization -- Numpy\n",
    "L1 regularization is the sum of the absolute values of the weights times a scaling constant, lambda.\n",
    "Below, you will define the function `l1_regularization` that accepts an input vector and a scalar constant, lambda.\n",
    "\n",
    "**NOTE:** We use the variable name `lamb` rather than `lambda` since `lambda` is a keyword in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db981120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, -2, 3, -4])\n",
    "np.abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "572cf0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(weights, lamb):\n",
    "    weights = np.abs(weights)\n",
    "    return lamb * np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f223a4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great work!\n"
     ]
    }
   ],
   "source": [
    "# Grading code. Run this cell to test your code!\n",
    "grading_vector = np.array([1, -2, 3, -4])\n",
    "assert l1_regularization(grading_vector, 0.5) == 5, f\"Your L1 regularization implementation seems to be incorrect. Expected 5, got {l1_regularization(grading_vector, 0.5)}\"\n",
    "assert l1_regularization(grading_vector, 1) == 10, f\"Your L1 regularization implementation seems to be incorrect. Expected 10, got {l1_regularization(grading_vector, 1)}\"\n",
    "\n",
    "print(\"Great work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504e737",
   "metadata": {},
   "source": [
    "## L2 Regularization -- Numpy\n",
    "L2 regularization squares the weights inside the vector and returns the sum of those squares times a scaling constant, lambda. \n",
    "Below, you will define the function `l2_regularization`, which accepts an input vector and a scalar constant, lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65b000de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(weights, lamb):\n",
    "    return lamb * np.sum(np.square(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f909714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great work!\n"
     ]
    }
   ],
   "source": [
    "# Grading code. Run this cell to test your code!\n",
    "grading_vector = np.array([0.5, -1, 1.5, -2])\n",
    "assert l2_regularization(grading_vector, 0.5) == 3.75, f\"Your L2 regularization implementation seems to be incorrect. Expected 3.75, got {l2_regularization(grading_vector, 0.5)}\"\n",
    "assert l2_regularization(grading_vector, 1) == 7.5, f\"Your L2 regularization implementation seems to be incorrect. Expected 7.5, got {l2_regularization(grading_vector, 1)}\"\n",
    "\n",
    "print(\"Great work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8472342",
   "metadata": {},
   "source": [
    "## Regularization in PyTorch\n",
    "Although L2 regularization is typically handled via the `weight_decay` parameter in your optimizer, we can compute L1 and L2 regularization by hand. \n",
    "We do this by iterating over the parameters in our model using the `net.parameters()` method.\n",
    "\n",
    "Rather than establishing a model, training it, and testing it, we will manually set the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e2d28fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[2., 2., 2., 2.]], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up our net for testing\n",
    "net = nn.Sequential(nn.Linear(4, 1, bias=False))\n",
    "# Make it so autograd doesn't track our changes\n",
    "with torch.no_grad():\n",
    "    net[0].weight = nn.Parameter(torch.ones_like(net[0].weight))\n",
    "    net[0].weight.fill_(2.0)\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f897d1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.], grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(net[0].weight[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d0399971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define L1 loss\n",
    "def l1_torch(model, lamb):\n",
    "    return lamb * sum([p.abs().sum() for p in model.parameters()])\n",
    "#     return lamb * torch.sum(torch.abs(model[0].weight[0, :]))\n",
    "    \n",
    "\n",
    "# Define L2 loss\n",
    "def l2_torch(model, lamb):\n",
    "    return lamb * sum([p.square().sum() for p in model.parameters()])\n",
    "#     return lamb * torch.sum(torch.square(model[0].weight[0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "16c4d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great work!\n"
     ]
    }
   ],
   "source": [
    "# Grading code\n",
    "assert l1_torch(net, 1) == 8, f\"There is something wrong with your L1 regularization implementation. Expected 8, got {l1_torch(net, 1)}\"\n",
    "assert l1_torch(net, 0.5) == 4, f\"There is something wrong with your L1 regularization implementation. Expected 4, got {l1_torch(net, 0.5)}\"\n",
    "\n",
    "assert l2_torch(net, 1) == 16, f\"There is something wrong with your L2 regularization implementation. Expected 16, got {l2_torch(net, 1)}\"\n",
    "assert l2_torch(net, 0.25) == 4, f\"There is something wrong with your L2 regularization implementation. Expected 4, got {l2_torch(net, 0.25)}\"\n",
    "\n",
    "print(\"Great work!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
