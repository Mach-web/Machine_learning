{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Performances\n",
    "\n",
    "Now that we have seen some tricks to improve performances of a CNN, let's apply them to the CIFAR10 dataset that we used in our previous exercise. \n",
    "\n",
    "To keep things simple, we will optimize two hyperparameters.\n",
    "\n",
    "For our data augmentation, we are going to use the `RandAugment` [auto-augmentation policy](https://pytorch.org/vision/main/generated/torchvision.transforms.RandAugment.html) which only takes one parameter, called `magnitude`. This parameter will be one of the parameters we will optimize in our hyperparameter search.\n",
    "\n",
    "We will also optimize the learning rate for the gradient descent. \n",
    "\n",
    "When doing hyperparameter optimization, it is important to have high-level functions (or scripts) that receive our hyperparameters as inputs, so we can run as many training runs as we need by only varying the calling to the function (or the script). Of course, this would not work if we were to hard-code the parameters.\n",
    "\n",
    "Ok, enough with the introduction, let's get started!\n",
    "\n",
    "As usual, let's start by installing our requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the kernel after running this cell.\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make sure that the GPU is active (otherwise things will be _very_ slow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Augmentation\n",
    "\n",
    "Here we write two functions that create appropriate transforms for the training, validation and test datasets, and then create the relative dataloaders.\n",
    "\n",
    "As usual, complete the code in the sections marked with `# YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "import multiprocessing\n",
    "from helpers_1 import get_train_val_data_loaders, get_test_data_loader\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "# Let's write a function that gives us the transforms so we can optimize the hyperparameters\n",
    "def get_transforms(rand_augment_magnitude):\n",
    "\n",
    "    # These are the per-channel mean and std of CIFAR-10 over the dataset\n",
    "    mean = (0.49139968, 0.48215827, 0.44653124)\n",
    "    std = (0.24703233, 0.24348505, 0.26158768)\n",
    "\n",
    "    # Define our transformations\n",
    "    return {\n",
    "        \"train\": T.Compose(\n",
    "            [\n",
    "                # All images in CIFAR-10 are 32x32. We enlarge them a bit so we can then\n",
    "                # take a random crop\n",
    "                T.Resize(40),\n",
    "                \n",
    "                # take a random part of the image\n",
    "                T.RandomCrop(32),\n",
    "                \n",
    "                # Horizontal flip is not part of RandAugment according to the RandAugment\n",
    "                # paper\n",
    "                T.RandomHorizontalFlip(0.5),\n",
    "                \n",
    "                # Use RandAugment\n",
    "                # RandAugment has 2 main parameters: how many transformations should be\n",
    "                # applied to each image, and the strength of these transformations. This\n",
    "                # latter parameter should be tuned through experiments: the higher the more\n",
    "                # the regularization effect.\n",
    "                # Setup a T.RandAugment transformation using 2 as num_opts, and the\n",
    "                # rand_augment_magnitude input parameter as magnitude. \n",
    "                # Use T.InterpolationMode.BILINEAR as interpolation. Look at the pytorch\n",
    "                # manual if needed: \n",
    "                # https://pytorch.org/vision/main/generated/torchvision.transforms.RandAugment.html\n",
    "            \n",
    "                # YOUR CODE HERE\n",
    "                T.RandAugment(num_ops = 2, magnitude = 9, interpolation = T.InterpolationMode.BILINEAR),\n",
    "                T.RandAffline(scale=(0.9, 1.1), interpolation = T.InterpolationMode.BILINEAR),\n",
    "                \n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean, std),\n",
    "            ]\n",
    "        ),\n",
    "        \"valid\": T.Compose(\n",
    "            [\n",
    "                # Both of these are useless, but we keep them because\n",
    "                # in a non-academic dataset you will need them\n",
    "                T.Resize(32),\n",
    "                T.CenterCrop(32),\n",
    "                \n",
    "                # Convert to tensor and apply normalization:\n",
    "                \n",
    "                # YOUR CODE HERE\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean, std),\n",
    "            ]\n",
    "        ),\n",
    "        # Identical to the valid set in this case\n",
    "        \"test\": T.Compose(\n",
    "            [\n",
    "                T.Resize(32),\n",
    "                T.CenterCrop(32),\n",
    "                \n",
    "                # Convert to tensor and apply normalization:\n",
    "                \n",
    "                # YOUR CODE HERE\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean, std),\n",
    "            ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_data_loaders(batch_size, valid_size, transforms, num_workers, random_seed=42):\n",
    "    \n",
    "    # Reseed random number generators to get a deterministic split. This is useful\n",
    "    # when comparing experiments, so you'll know they all run on the same data.\n",
    "    # In principle you should repeat this a few times (cross validation) to see\n",
    "    # the variability of your measurements, but we won't do this here for simplicity\n",
    "    torch.manual_seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    DATA = \"C:\\\\Users\\\\mkand\\\\Documents\\\\Machine_learning\\\\Datasets\\\\cifar-10-python\"\n",
    "    # Get the CIFAR10 training dataset from torchvision.datasets and set the transforms\n",
    "    # We will split this further into train and validation in this function\n",
    "    train_data = datasets.CIFAR10(DATA, train=True, transform=transforms['train'])\n",
    "    valid_data = datasets.CIFAR10(DATA, train=True, transform=transforms['valid'])\n",
    "\n",
    "    # Compute how many items we will reserve for the validation set\n",
    "    n_tot = len(train_data)\n",
    "    split = int(np.floor(valid_size * n_tot))\n",
    "\n",
    "    # compute the indices for the training set and for the validation set\n",
    "    shuffled_indices = torch.randperm(n_tot)\n",
    "    train_idx, valid_idx = shuffled_indices[split:], shuffled_indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # prepare data loaders (combine dataset and sampler)\n",
    "    # NOTE that here we use train_data for the train dataloader but valid_data\n",
    "    # for the valid_loader, so the respective transforms are applied\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    test_data = datasets.CIFAR10(DATA, train=False, transform=transforms['test'])\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=batch_size, num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return {'train': train_loader, 'valid': valid_loader, 'test': test_loader}\n",
    "\n",
    "# specify the image classes\n",
    "classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "Here we use a model very similar to the one we used before, but we add Batch Normalization that makes our training faster and more robust, and also allows us to go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "            # Add batch normalization (BatchNorm2d) here\n",
    "            # YOUR CODE HERE\n",
    "            nn.BatchNorm2D(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, 3, padding=1),  # -> 32x16x16\n",
    "            # Add batch normalization (BatchNorm2d) here\n",
    "            # YOUR CODE HERE\n",
    "            nn.BatchNorm2D(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # -> 32x8x8\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),  # -> 64x8x8\n",
    "            # Add batch normalization (BatchNorm2d) here\n",
    "            # YOUR CODE HERE\n",
    "            nn.BatchNorm2D(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # -> 64x4x4\n",
    "            \n",
    "            # Since we are using BatchNorm and data augmentation,\n",
    "            # we can go deeper than before and add one more conv layer\n",
    "            nn.Conv2d(64, 128, 3, padding=1),  # -> 128x4x4\n",
    "            # Add batch normalization (BatchNorm2d) here\n",
    "            # YOUR CODE HERE\n",
    "            nn.BatchNorm2d(128)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # -> 128x2x2\n",
    "            \n",
    "            nn.Flatten(),  # -> 1x128x2x2\n",
    "            \n",
    "            nn.Linear(128 * 2 * 2, 500),  # -> 500\n",
    "            nn.BatchNorm\n",
    "            nn.Dropout(0.5),\n",
    "            # Add batch normalization (BatchNorm1d, NOT BatchNorm2d) here\n",
    "            # YOUR CODE HERE\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Just call the model on x here:\n",
    "        # YOUR CODE HERE\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Learning Rate Finder\n",
    "\n",
    "Before we start our training, let's find a range for the learning rate that makes sense for our situation. We will use the learning rate finder that we've seen in one of the previous videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to look into the code of lr_finder and see how it works!\n",
    "from lr_finder import lr_finder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 64\n",
    "valid_size = 0.2\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "transforms = get_transforms(rand_augment_magnitude=9)\n",
    "data_loaders = get_data_loaders(batch_size, valid_size, transforms, num_workers)\n",
    "\n",
    "# Range  and number of steps for the learning rate\n",
    "min_lr = 1e-5\n",
    "max_lr = 1\n",
    "n_steps = min(len(data_loaders['train']), 200)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = lr_finder(min_lr, max_lr, n_steps, loss, model, data_loaders)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(losses.keys(), losses.values())\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"learning rate (log scale)\")\n",
    "\n",
    "# Adjust the range on the y-axis to see things more clearly\n",
    "plt.xlim([1e-4, None])\n",
    "plt.ylim([min(losses.values()), np.percentile(list(losses.values()), 97)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that a good initial choice for the learning rate is in the middle of the steep part. In this case it seems that 0.04 is a good initial choice.\n",
    "\n",
    "# Learning Rate Scheduler + Hyperparameter Optimization\n",
    "Let's also use two other tricks we have just learned: the learning rate scheduler, that changes the learning rate as the training progresses, and the hyperparameter optimization that optimizes the choices to maximize performance.\n",
    "\n",
    "Let's start by writing an optimize function that leverages the Learning Rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot\n",
    "from helpers import train_one_epoch, valid_one_epoch\n",
    "import torch.optim\n",
    "\n",
    "\n",
    "def optimize(data_loaders, model, optimizer, loss, n_epochs, save_path, interactive_tracking=False):\n",
    "    \n",
    "    # This is a plotting function\n",
    "    def after_subplot(ax: plt.Axes, group_name: str, x_label: str):\n",
    "        \"\"\"Add title xlabel and legend to single chart\"\"\"\n",
    "        ax.set_title(group_name)\n",
    "        ax.set_xlabel(x_label)\n",
    "        ax.legend(loc=\"center right\")\n",
    "        \n",
    "    # initialize tracker for minimum validation loss\n",
    "    if interactive_tracking:\n",
    "        liveloss = PlotLosses(outputs=[MatplotlibPlot(after_subplot=after_subplot)])\n",
    "    else:\n",
    "        liveloss = None\n",
    "\n",
    "    valid_loss_min = None\n",
    "    logs = {}\n",
    "\n",
    "    # Learning rate scheduler: setup a learning rate scheduler that\n",
    "    # reduces the learning rate when the validation loss reaches a\n",
    "    # plateau. Use torch.optim.lr_scheduler.ReduceLROnPlateau, with\n",
    "    # a treshold of 0.01. Look at the docs if in doubt:\n",
    "    # https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        train_loss = train_one_epoch(\n",
    "            data_loaders[\"train\"], model, optimizer, loss\n",
    "        )\n",
    "\n",
    "        valid_loss = valid_one_epoch(data_loaders[\"valid\"], model, loss)\n",
    "\n",
    "        # If the validation loss decreases by more than 1%, save the model\n",
    "        if valid_loss_min is None or (\n",
    "                (valid_loss_min - valid_loss) / valid_loss_min > 0.01\n",
    "        ):\n",
    "\n",
    "            # Save the weights to save_path\n",
    "            torch.save(model.state_dict(), save_path)  # -\n",
    "\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "        # Update learning rate, i.e., make a step in the learning rate scheduler\n",
    "        # Remember to use the validation loss, so that the learning rate scheduler\n",
    "        # will change the learning rate when the validation loss is not \n",
    "        # decreasing anymore\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Log the losses and the current learning rate\n",
    "        if interactive_tracking:\n",
    "            logs[\"loss\"] = train_loss\n",
    "            logs[\"val_loss\"] = valid_loss\n",
    "            logs[\"lr\"] = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            liveloss.update(logs)\n",
    "            liveloss.send()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function that trains and validates our model given some hyperparameters. Let's consider for simplicity just two hyperparameters: the learning rate and the strength of the data augmentation in `RandAugment`.\n",
    "\n",
    "We are also going to track our experiments with `mlflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow \n",
    "from helpers import one_epoch_test\n",
    "\n",
    "def train_one_model(learning_rate, rand_augment_magnitude, n_epochs):\n",
    "    \n",
    "    transforms = get_transforms(rand_augment_magnitude=rand_augment_magnitude)\n",
    "    data_loaders = get_data_loaders(batch_size, valid_size, transforms, num_workers)\n",
    "    model = Net()\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        model.cuda()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        optimize(data_loaders, model, optimizer, loss, n_epochs, \"best_val_loss.pt\", interactive_tracking=True)\n",
    "        \n",
    "        # Restore best validation loss\n",
    "        model.load_state_dict(torch.load('best_val_loss.pt'))\n",
    "        \n",
    "        # Test model on *validation* set (never optimize your hyperparameters on the \n",
    "        # test set!)\n",
    "        val_loss, preds, actuals = one_epoch_test(data_loaders['valid'], model, loss)\n",
    "        \n",
    "        # Use mlflow.log_param to log the learning rate and the\n",
    "        # rand_augment_magnitude\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Use mlflow.log_metric to log your validation loss\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        val_accuracy = (np.array(preds)==np.array(actuals)).sum() / len(actuals)\n",
    "        \n",
    "        # Use mlflow to log the validation accuracy as a metric\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Use mlflow.log_artifact to log the best_val_loss.pt file\n",
    "        # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "Let's use the random search technique to explore our hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a random grid of learning rate and rand_augment_magnitude\n",
    "min_lr = 0.01\n",
    "max_lr = 0.1\n",
    "\n",
    "# Here you can decide how many experiments to run\n",
    "# Run between 5 and 20. The more you run the better\n",
    "# your results might be, but of course it will take\n",
    "# longer\n",
    "# Normally you would use a lot more than that, which is why\n",
    "# you typically do hyperparam optimization in the cloud so \n",
    "# all the experiments can run in parallel\n",
    "n_grid = # YOUR CODE HERE\n",
    "\n",
    "# Sample log-uniformly the learning rate\n",
    "lrs = 10**(np.random.uniform(np.log10(min_lr), np.log10(max_lr), n_grid))\n",
    "# Sample uniformly the rand augment transform strength\n",
    "r_a_mag = np.random.randint(1, 15, n_grid)\n",
    "\n",
    "# Plot our grid\n",
    "_ = plt.scatter(lrs, r_a_mag)\n",
    "_ = plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our experiments\n",
    "for lr, rand_aug_mag in zip(lrs, r_a_mag):\n",
    "    train_one_model(lr, rand_aug_mag, n_epochs=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the results of our experiments. If you are running locally, you could now run `mlflow ui` to explore the results. If you are in the Udacity workspace, use the following code that reads the results and return them as a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "runs = mlflow.search_runs()\n",
    "sorted_runs = runs[\n",
    "    [\n",
    "        \"run_id\",\n",
    "        \"params.learning_rate\",\n",
    "        \"params.rand_augment_magnitude\",\n",
    "        \"metrics.val_loss\",\n",
    "        \"metrics.val_accuracy\",\n",
    "    ]\n",
    "].sort_values(by='metrics.val_loss')\n",
    "sorted_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Get the id with the lowest val_loss\n",
    "lowest_loss_id = sorted_runs.iloc[0]['run_id']\n",
    "\n",
    "# Fetch the best model from that run\n",
    "client = MlflowClient()\n",
    "local_path = client.download_artifacts(lowest_loss_id, \"best_val_loss.pt\", '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_val_loss.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network\n",
    "\n",
    "Test your trained model on previously unseen data! A \"good\" result will be a CNN that gets around 70% (or more, try your best!) accuracy on these test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import one_epoch_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Accuracy: 77% (7701/10000)\n",
    "\n",
    "test_loss, preds, actuals = one_epoch_test(data_loaders['test'], model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plot_confusion_matrix\n",
    "\n",
    "cm = plot_confusion_matrix(preds, actuals, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have improved on our previous results by almost 5%. There is a lot more that can be done, feel free to keep experimenting with different augmentations for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
